{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive \n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "df=pd.read_csv('sentiment_5_class.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>injects just enough freshness into the proceed...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>never plays as dramatic even when dramatic thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None of this is very original , and it is n't ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, Madonna gives her best performance since Abe...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The most compelling performance of the year</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stirring visual sequence</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is a smart movie that knows its classical musi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>budding demons</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>such a good job</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>you 're entirely unprepared .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>credits</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LaBute continues to improve .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>This is popcorn movie fun with equal doses of ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>An odd drama set in the world of lingerie mode...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>it makes up for with a great , fiery passion .</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wanted so badly for the protagonist</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>superb performance</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gutsy and</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hugely imaginative and successful casting</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>its Oscar-sweeping franchise predecessor</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>energizing</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>All About the Benjamins evokes the bottom tier...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a decent sense of humor and plenty</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>, endearing , masterful</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>larger than life</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>coming down off of Miramax 's deep shelves aft...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A smart and funny , albeit sometimes superfici...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>life</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ponder the whole notion of passion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18359</th>\n",
       "      <td>his most sparkling</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18360</th>\n",
       "      <td>Steven Spielberg 's</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18361</th>\n",
       "      <td>is hamstrung by a badly handled screenplay of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18362</th>\n",
       "      <td>authentic feel</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18363</th>\n",
       "      <td>is a monumental achievement in practically eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18364</th>\n",
       "      <td>is clever enough ,</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18365</th>\n",
       "      <td>sleeping dogs</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18366</th>\n",
       "      <td>with the aid of those wisecracking Mystery Sci...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18367</th>\n",
       "      <td>is cruel , misanthropic stuff with only weak c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18368</th>\n",
       "      <td>photographed and staged by Mendes with a serie...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18369</th>\n",
       "      <td>caffeinated , sloppy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18370</th>\n",
       "      <td>quite good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18371</th>\n",
       "      <td>that elevate `` Glory '' above most of its ilk</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18372</th>\n",
       "      <td>marvelous series</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18373</th>\n",
       "      <td>excellent cast</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18374</th>\n",
       "      <td>those who paid for it</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18375</th>\n",
       "      <td>Less an examination of neo-Nazism than a probe...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18376</th>\n",
       "      <td>Mile</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18377</th>\n",
       "      <td>with an original idea for a teen movie</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18378</th>\n",
       "      <td>ride</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18379</th>\n",
       "      <td>the bravery and dedication</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18380</th>\n",
       "      <td>refresh</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18381</th>\n",
       "      <td>intense and engrossing head-trip</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18382</th>\n",
       "      <td>of corny dialogue and preposterous moments</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18383</th>\n",
       "      <td>Its initial excitement</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18384</th>\n",
       "      <td>to balance pointed , often incisive satire and...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18385</th>\n",
       "      <td>have to be a most hard-hearted person not to b...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18386</th>\n",
       "      <td>could young romantics out on a date</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18387</th>\n",
       "      <td>could be this good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18388</th>\n",
       "      <td>such a dungpile</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18389 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Phrase  Sentiment\n",
       "0      injects just enough freshness into the proceed...          3\n",
       "1                                                   that          2\n",
       "2      never plays as dramatic even when dramatic thi...          0\n",
       "3      None of this is very original , and it is n't ...          0\n",
       "4      , Madonna gives her best performance since Abe...          3\n",
       "5            The most compelling performance of the year          4\n",
       "6                               stirring visual sequence          3\n",
       "7      is a smart movie that knows its classical musi...          4\n",
       "8                                         budding demons          2\n",
       "9                                        such a good job          4\n",
       "10                         you 're entirely unprepared .          2\n",
       "11                                               credits          2\n",
       "12                         LaBute continues to improve .          3\n",
       "13     This is popcorn movie fun with equal doses of ...          3\n",
       "14     An odd drama set in the world of lingerie mode...          4\n",
       "15        it makes up for with a great , fiery passion .          4\n",
       "16                   wanted so badly for the protagonist          3\n",
       "17                                    superb performance          4\n",
       "18                                             gutsy and          3\n",
       "19             hugely imaginative and successful casting          4\n",
       "20              its Oscar-sweeping franchise predecessor          4\n",
       "21                                            energizing          4\n",
       "22     All About the Benjamins evokes the bottom tier...          0\n",
       "23                    a decent sense of humor and plenty          4\n",
       "24                               , endearing , masterful          4\n",
       "25                                      larger than life          3\n",
       "26     coming down off of Miramax 's deep shelves aft...          1\n",
       "27     A smart and funny , albeit sometimes superfici...          4\n",
       "28                                                  life          3\n",
       "29                    ponder the whole notion of passion          3\n",
       "...                                                  ...        ...\n",
       "18359                                 his most sparkling          4\n",
       "18360                                Steven Spielberg 's          2\n",
       "18361  is hamstrung by a badly handled screenplay of ...          0\n",
       "18362                                     authentic feel          3\n",
       "18363  is a monumental achievement in practically eve...          1\n",
       "18364                                 is clever enough ,          3\n",
       "18365                                      sleeping dogs          2\n",
       "18366  with the aid of those wisecracking Mystery Sci...          2\n",
       "18367  is cruel , misanthropic stuff with only weak c...          0\n",
       "18368  photographed and staged by Mendes with a serie...          3\n",
       "18369                               caffeinated , sloppy          0\n",
       "18370                                         quite good          4\n",
       "18371     that elevate `` Glory '' above most of its ilk          3\n",
       "18372                                   marvelous series          4\n",
       "18373                                     excellent cast          4\n",
       "18374                              those who paid for it          2\n",
       "18375  Less an examination of neo-Nazism than a probe...          3\n",
       "18376                                               Mile          2\n",
       "18377             with an original idea for a teen movie          3\n",
       "18378                                               ride          2\n",
       "18379                         the bravery and dedication          3\n",
       "18380                                            refresh          3\n",
       "18381                   intense and engrossing head-trip          4\n",
       "18382         of corny dialogue and preposterous moments          0\n",
       "18383                             Its initial excitement          3\n",
       "18384  to balance pointed , often incisive satire and...          3\n",
       "18385  have to be a most hard-hearted person not to b...          4\n",
       "18386                could young romantics out on a date          3\n",
       "18387                                 could be this good          3\n",
       "18388                                    such a dungpile          0\n",
       "\n",
       "[18389 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "Dataset split in train and test set\n",
    "KFold Cross Validation used to determine hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of instances 18389\n",
      "[0 1 2 3 4]\n",
      "[1235 1456 2345 8792 4561]\n"
     ]
    }
   ],
   "source": [
    "X=df.Phrase.tolist()\n",
    "print('No of instances',len(X))\n",
    "y=df.Sentiment.tolist()\n",
    "values, counts = np.unique(y, return_counts=True)\n",
    "print(values)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14711\n",
      "3678\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "# print(y_train)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "**Binary Feature Vectorization<br>\n",
    "Count Feature Vectorization<br>\n",
    "TF-IDF Feature Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['Happy, happy','I am so happy','Sad sad', 'I am so sad']\n",
    "c_vectorizer = CountVectorizer(token_pattern='(?u)\\\\b\\\\w\\\\w*\\\\b')\n",
    "t_vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w\\\\w*\\\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'happy', 'i', 'sad', 'so']\n",
      "['am', 'happy', 'i', 'sad', 'so']\n"
     ]
    }
   ],
   "source": [
    "c_vectorizer.fit(data)\n",
    "print(c_vectorizer.get_feature_names())\n",
    "\n",
    "t_vectorizer.fit(data)\n",
    "print(t_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w*\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w*\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data into vector is done only by transform\n",
    "['am', 'happy', 'i', 'sad', 'so']\n",
    "and its respective frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 2, 0],\n",
       "       [1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_data_v=c_vectorizer.transform(data)  #Vector conversion\n",
    "\n",
    "c_data_v.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am', 'happy', 'sad', 'so']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_vectorizer = TfidfVectorizer()  # (token pattern ='(?u)\\\\b\\\\w\\\\w*\\\\b')\n",
    "t_vectorizer.fit(data)\n",
    "t_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 1. , 0. , 0. , 0. ],\n",
       "       [0.5, 0.5, 0.5, 0. , 0.5],\n",
       "       [0. , 0. , 0. , 1. , 0. ],\n",
       "       [0.5, 0. , 0.5, 0.5, 0.5]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data_v=t_vectorizer.transform(data)\n",
    "t_data_v.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "# Fit should be done only on training data BE CAREFULLLLL\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v=vectorizer.transform(X_train)\n",
    "X_test_v= vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=SVC()\n",
    "model.fit(X_train_v,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# There are three classes [0,1,2]\n",
    "a_true = [1,1,1]\n",
    "a_pred = [1,1,0]\n",
    "\n",
    "# Confusion matrix\n",
    "# [0, 0, 0\n",
    "#  1, 2, 0\n",
    "#  0, 0, 0]\n",
    "\n",
    "TP_0 = 0\n",
    "FP_0 = 1\n",
    "Precision_0 = 0\n",
    "\n",
    "TP_1 = 0\n",
    "FP_1 = 1\n",
    "Precision_1 = 1\n",
    "\n",
    "TP_2 = 0\n",
    "FP_2 = 0\n",
    "Precision_2 = 0\n",
    "\n",
    "macro_precision = (Precision_0 + Precision_1 + Precision_2)/3\n",
    "print(macro_precision)\n",
    "\n",
    "#Preferred\n",
    "micro_precision = (TP_0 + TP_1 + TP_2)/(TP_0 + FP_0 + TP_1 + FP_1 + TP_2 + FP_2)\n",
    "print(micro_precision)\n",
    "\n",
    "weight_precision = (Precision_0*0 + Precision_1*3 + Precision_2*0)/(0+3+0)\n",
    "print(weight_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.67      0.80         3\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.67      0.67      0.67         3\n",
      "   macro avg       0.33      0.22      0.27         3\n",
      "weighted avg       1.00      0.67      0.80         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkarki/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jkarki/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(a_true, a_pred, labels=[0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.54      0.66       247\n",
      "           1       0.70      0.47      0.56       291\n",
      "           2       0.67      0.51      0.58       469\n",
      "           3       0.67      0.88      0.76      1759\n",
      "           4       0.79      0.55      0.65       912\n",
      "\n",
      "    accuracy                           0.70      3678\n",
      "   macro avg       0.73      0.59      0.64      3678\n",
      "weighted avg       0.71      0.70      0.69      3678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_v)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5,6,7,8,9,10]  #X_train and test\n",
    "y = [1,0,0,0,1,1,1,1,1,0]   #y_train and test\n",
    "\n",
    "# k fold cross validation\n",
    "K = 5\n",
    "x_1 = [1, 2]\n",
    "y_1 = [1, 0]\n",
    "\n",
    "x_2 = [3, 4]\n",
    "y_2 = [0, 0]\n",
    "\n",
    "x_3 = [5, 6]\n",
    "y_3 = [1, 1]\n",
    "\n",
    "x_4 = [7, 8]\n",
    "y_4 = [1, 1]\n",
    "\n",
    "x_5 = [9, 10]\n",
    "y_5 = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 6 8 9] [1 1 0 2 2]\n",
      "[ 1  3  5  7 10] [1 1 3 0 3]\n",
      "[ 1  3  5  7 10] [1 1 3 0 3]\n",
      "[2 4 6 8 9] [1 1 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "a=np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "b=np.array([1,1,1,1,3,0,0,2,2,3])\n",
    "\n",
    "k=2\n",
    "skf=KFold(n_splits=k, shuffle=True,random_state=1)\n",
    "\n",
    "for train_index, test_index in skf.split(a,b):\n",
    "    print(a[train_index],b[train_index])\n",
    "    print(a[test_index],b[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  7  9 10] [1 1 0 2 3]\n",
      "[3 4 5 6 8] [1 1 3 0 2]\n",
      "[3 4 5 6 8] [1 1 3 0 2]\n",
      "[ 1  2  7  9 10] [1 1 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "a=np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "b=np.array([1,1,1,1,3,0,0,2,2,3])\n",
    "\n",
    "k=2\n",
    "\n",
    "skf=StratifiedKFold(n_splits=k, shuffle=True,random_state=1)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(a,b):\n",
    "    print(a[train_index],b[train_index])\n",
    "    print(a[test_index],b[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': ('linear', 'rbf'), 'C': (1, 10)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_param={'kernel':('linear','rbf'),'C':(1,10)}\n",
    "grid_param\n",
    "\n",
    "#Makes a combination [linear,1],[linear,10],[rbf,1],[rbf,10]\n",
    "\n",
    "#[linear,1] --> 5 fold stratified cross validation, average_val_loss_calculate\n",
    "#[linear,10] --> 5 fold cross validation, average_val_loss_calculate\n",
    "#[rbf,1] --> 5 fold cross validation, average_val_loss_calculate\n",
    "#[rbf,10] --> 5 fold cross validation, average_val_loss_calculate\n",
    "\n",
    "\n",
    "#Define scorer in scoring=None for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "scorer=make_scorer(f1_score,average='micro')\n",
    "clf=GridSearchCV(SVC(),grid_param,scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': (1, 10), 'kernel': ('linear', 'rbf')},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(f1_score, average=micro), verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf=GridSearchCV(SVC(),grid_param)\n",
    "clf.fit(X_train_v,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6938339439608625 {'C': 10, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_score_,clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.67      0.71       247\n",
      "           1       0.68      0.58      0.62       291\n",
      "           2       0.65      0.63      0.64       469\n",
      "           3       0.72      0.81      0.76      1759\n",
      "           4       0.71      0.61      0.65       912\n",
      "\n",
      "    accuracy                           0.71      3678\n",
      "   macro avg       0.70      0.66      0.68      3678\n",
      "weighted avg       0.71      0.71      0.70      3678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Final Model\n",
    "\n",
    "model=SVC(random_state=1,kernel='rbf',C=10)\n",
    "model.fit(X_train_v,y_train)\n",
    "y_pred=model.predict(X_test_v)\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.56      0.65       247\n",
      "           1       0.63      0.47      0.54       291\n",
      "           2       0.65      0.51      0.57       469\n",
      "           3       0.68      0.86      0.76      1759\n",
      "           4       0.75      0.56      0.64       912\n",
      "\n",
      "    accuracy                           0.69      3678\n",
      "   macro avg       0.70      0.59      0.63      3678\n",
      "weighted avg       0.69      0.69      0.68      3678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=SVC(random_state=1,kernel='linear',C=1)\n",
    "model.fit(X_train_v,y_train)\n",
    "y_pred=model.predict(X_test_v)\n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "\n",
    "#Report the f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
